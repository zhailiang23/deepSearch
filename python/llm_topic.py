import requests
import json
import logging
from typing import List, Dict, Optional
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('cluster_analysis.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def generate_cluster_description(cluster_texts: List[str], api_key: str) -> Optional[Dict]:
    """è°ƒç”¨QWen2-72Bæ¨¡å‹ç”Ÿæˆç°‡æè¿°ï¼Œé‡ç‚¹çªå‡ºç”¨æˆ·å…·ä½“é—®é¢˜"""
    if not cluster_texts:
        logger.warning("èšç±»æ–‡æœ¬ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆæè¿°")
        return None
        
    # é™åˆ¶è¾“å…¥æ–‡æœ¬æ•°é‡ï¼Œé¿å…è¿‡é•¿
    sample_texts = cluster_texts[:10]
    prompt = f"""ä½œä¸ºé“¶è¡Œå®¢æˆ·åé¦ˆåˆ†æä¸“å®¶ï¼Œè¯·ç²¾å‡†æç‚¼ç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜ï¼š
    
    ã€è¾“å…¥æ•°æ®ã€‘
    ä»¥ä¸‹æ˜¯åŒä¸€ç±»åˆ«çš„ç”¨æˆ·åé¦ˆï¼š
    {chr(10).join([f'- {text}' for text in sample_texts])}

    ã€è¾“å‡ºè¦æ±‚ã€‘
    1. ç”Ÿæˆ1ä¸ªç®€æ´çš„ç±»åˆ«åç§°ï¼ˆ10-15å­—ï¼‰ï¼Œå¿…é¡»åŒ…å«ï¼š
       - æ¶‰åŠçš„ä¸šåŠ¡é¢†åŸŸï¼ˆå¦‚è½¬è´¦ã€ä¿¡ç”¨å¡ã€APPç­‰ï¼‰
       - ç”¨æˆ·çš„æ ¸å¿ƒè¯‰æ±‚æˆ–é—®é¢˜ï¼ˆå¦‚é¢åº¦ä¸è¶³ã€æ— æ³•ä½¿ç”¨ã€æµç¨‹å¤æ‚ç­‰ï¼‰
       ç¤ºä¾‹ï¼š"è½¬è´¦é™é¢ä¸è¶³ï¼Œå¸Œæœ›æé«˜"ã€"ä¿¡ç”¨å¡å®¡æ‰¹è¢«æ‹’ï¼Œè¯¢é—®åŸå› "
    
    2. åˆ—å‡º3ä¸ªæœ€å…·ä»£è¡¨æ€§çš„åŸå§‹é—®é¢˜ï¼ˆä¿ç•™ç”¨æˆ·è¡¨è¿°ç‰¹ç‚¹ï¼‰
    
    3. æå–2-3ä¸ªä¸šåŠ¡æ ‡ç­¾ï¼ˆå¦‚"è½¬è´¦é™é¢"ã€"ä¿¡ç”¨å¡å®¡æ‰¹"ã€"APPæ•…éšœ"ï¼‰
    
    è¯·ç”¨JSONæ ¼å¼å›å¤ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬ï¼š
    {{
        "topic": "ç±»åˆ«åç§°",
        "examples": ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"],
        "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"]
    }}"""
    
    try:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        data = {
            "model": "QWen2-72B-Instruct",
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯æœ‰10å¹´ç»éªŒçš„é“¶è¡Œä¸šåŠ¡åˆ†æå¸ˆï¼Œæ“…é•¿ä»ç”¨æˆ·åé¦ˆä¸­æç‚¼å…·ä½“é—®é¢˜å’Œè¯‰æ±‚"},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3,
            "max_tokens": 500
        }
        
        logger.info(f"è°ƒç”¨å¤§æ¨¡å‹å¤„ç†èšç±»æ–‡æœ¬ï¼Œæ ·æœ¬æ•°é‡: {len(sample_texts)}")
        response = requests.post(
            "http://192.168.156.142:9902/QWen2-72B-Instruct/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        
        response.raise_for_status()
        result = response.json()
        
        if 'choices' in result and len(result['choices']) > 0:
            content = result['choices'][0]['message']['content'].strip()
            if content.startswith('```json'):
                content = content[7:-3].strip()
            return json.loads(content)
        else:
            logger.error("APIè¿”å›æ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œç¼ºå°‘choiceså­—æ®µ")
            return None
            
    except Exception as e:
        logger.error(f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
        return None


def tfidf_fallback(cluster_texts: List[str], cluster_id: int) -> Dict:
    """TF-IDFé™çº§æ–¹æ¡ˆï¼Œå½“LLMè°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨"""
    try:
        tfidf = TfidfVectorizer(
            tokenizer=jieba.lcut,
            max_features=10,
            ngram_range=(1, 2),
            stop_words=['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰']
        )
        tfidf.fit_transform(cluster_texts)
        keywords = tfidf.get_feature_names_out()
        
        # å°è¯•ä»å…³é”®è¯ä¸­ç»„åˆå‡ºåŒ…å«é—®é¢˜çš„è¯é¢˜åç§°
        if len(keywords) >= 2:
            topic = f"{keywords[0]} {keywords[1]} é—®é¢˜"
        else:
            topic = "ã€".join(keywords) + "ç›¸å…³é—®é¢˜"
            
        return {
            "topic": topic[:15],  # æ§åˆ¶é•¿åº¦
            "examples": cluster_texts[:3],
            "tags": keywords[:3].tolist() if len(keywords)>=3 else keywords.tolist()
        }
    except Exception as e:
        logger.error(f"TF-IDFé™çº§æ–¹æ¡ˆå¤±è´¥: {str(e)}")
        return {
            "topic": f"ç°‡{cluster_id}çš„ç”¨æˆ·é—®é¢˜",
            "examples": cluster_texts[:3] if cluster_texts else [],
            "tags": ["ç”¨æˆ·é—®é¢˜"]
        }


def generate_all_cluster_descriptions(df_all, api_key: str) -> Dict:
    """ä¸ºæ‰€æœ‰æœ‰æ•ˆèšç±»ç”Ÿæˆæè¿°"""
    cluster_results = {}
    
    try:
        valid_clusters = sorted([cid for cid in df_all['cluster'].unique() if cid != -1])
    except KeyError:
        logger.error("DataFrameä¸­ç¼ºå°‘'cluster'åˆ—")
        return cluster_results

    if len(valid_clusters) == 0:
        logger.warning("æœªå‘ç°æœ‰æ•ˆèšç±»ï¼ˆæ‰€æœ‰ç‚¹å‡ä¸ºå™ªå£°ï¼‰")
        return cluster_results
        
    logger.info(f"å¼€å§‹ä¸º {len(valid_clusters)} ä¸ªçƒ­ç‚¹ç°‡ç”Ÿæˆæè¿°ï¼š{valid_clusters}")

    for cluster_id in valid_clusters:
        try:
            cluster_texts = df_all[df_all['cluster'] == cluster_id]['text'].tolist()
        except KeyError:
            logger.error("DataFrameä¸­ç¼ºå°‘'text'åˆ—")
            continue
            
        n_texts = len(cluster_texts)
        logger.info(f"å¤„ç†ç°‡ {cluster_id}ï¼ˆå…± {n_texts} æ¡åé¦ˆï¼‰")
        
        if n_texts == 0:
            logger.warning(f"ç°‡ {cluster_id} æ— æ–‡æœ¬æ•°æ®")
            continue

        result = generate_cluster_description(cluster_texts, api_key)
        
        if result and all(key in result for key in ['topic', 'examples', 'tags']):
            cluster_results[cluster_id] = result
        else:
            logger.info(f"ç°‡ {cluster_id} ä½¿ç”¨TF-IDFé™çº§ç”Ÿæˆæè¿°")
            cluster_results[cluster_id] = tfidf_fallback(cluster_texts, cluster_id)
    
    return cluster_results


def print_analysis_report(cluster_results: Dict):
    """æ‰“å°æ ¼å¼åŒ–çš„èšç±»åˆ†ææŠ¥å‘Š"""
    print("\n" + "ğŸ¦" + " " + "é“¶è¡Œç”¨æˆ·åé¦ˆèšç±»åˆ†ææŠ¥å‘Š".center(46) + " " + "ğŸ¦")
    print("="*60)
    
    if not cluster_results:
        print("âŒ æœªèƒ½ç”Ÿæˆä»»ä½•èšç±»æè¿°ï¼Œè¯·æ£€æŸ¥ API è¿æ¥æˆ–æ•°æ®ã€‚")
    else:
        for cluster_id, desc in sorted(cluster_results.items()):
            print(f"\nâ–  çƒ­ç‚¹ç°‡ {cluster_id}: {desc['topic']}")
            print(f"  â”œâ”€ ğŸ”– ä¸šåŠ¡æ ‡ç­¾: {', '.join(desc['tags'])}")
            print(f"  â””â”€ ğŸ’¬ å…¸å‹é—®é¢˜:")
            
            examples = desc.get('examples', [])
            for example in examples[:3]:
                if isinstance(example, str):
                    display_text = example.strip()
                    if len(display_text) > 60:
                        display_text = display_text[:57] + "..."
                    print(f"      â–ª {display_text}")

    print("\n" + "ğŸ’¡ å»ºè®®ï¼šä»¥ä¸Šçƒ­ç‚¹ç°‡å¯ä¼˜å…ˆå®‰æ’äº§å“ä¼˜åŒ–æˆ–å®¢æœè¯æœ¯æ›´æ–°ã€‚")
    print("="*60)


if __name__ == "__main__":
    api_key = "yk-4a72dfebe1a4455fb04e4e07c82c3d18"
    
    import pandas as pd
    df_all = pd.DataFrame({
        'cluster': [0, 0, 1, 1, -1],
        'text': [
            "æˆ‘çš„é“¶è¡Œå¡è¢«å†»ç»“äº†ï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆ",
            "é“¶è¡Œå¡çªç„¶ä¸èƒ½ç”¨äº†ï¼Œæ˜¾ç¤ºå†»ç»“çŠ¶æ€",
            "æ‰‹æœºé“¶è¡Œè½¬è´¦é™é¢å¤ªä½ï¼Œä¸å¤Ÿç”¨",
            "å¸Œæœ›èƒ½æé«˜æ‰‹æœºé“¶è¡Œçš„è½¬è´¦é¢åº¦",
            "è¿™ä¸ªé“¶è¡Œçš„æœåŠ¡çœŸå·®"
        ]
    })
    
    cluster_results = generate_all_cluster_descriptions(df_all, api_key)
    print_analysis_report(cluster_results)
    