# Task 008: 数据导入集成服务

## Metadata
```yaml
task_id: 008
name: 数据导入集成服务
description: 创建DataImportService，实现Elasticsearch索引创建和批量数据导入功能
epic: search-space-json-import
created_at: 2025-09-24T01:57:19Z
updated_at: 2025-09-24T02:23:45Z
estimated_hours: 10
priority: high
parallel: false
depends_on: [29]
tags: [backend, elasticsearch, data-import, bulk-processing, async]
github: https://github.com/zhailiang23/deepSearch/issues/31
status: todo
assignee: null
```

## Background
实现完整的数据导入集成服务，将JSON分析结果和索引配置整合，执行Elasticsearch索引创建和大规模批量数据导入。该服务需要处理异步任务、进度跟踪、错误恢复和性能优化，确保大数据集的稳定导入和良好的用户体验。

## Objectives
- 创建DataImportService数据导入核心服务
- 实现Elasticsearch索引创建和数据导入
- 集成异步任务处理和进度跟踪机制
- 提供错误处理和导入恢复功能
- 优化批量导入性能和内存使用

## Technical Requirements

### 核心服务类设计
- `DataImportService`: 数据导入主服务
- `ElasticsearchIndexManager`: ES索引管理器
- `BulkDataImporter`: 批量数据导入器
- `ImportProgressTracker`: 导入进度跟踪器
- `ImportTaskManager`: 导入任务管理器
- `ImportResult`: 导入结果模型

### 异步任务处理
- 使用Spring的@Async注解实现异步导入
- TaskExecutor线程池配置优化
- 导入状态实时更新和持久化
- 任务取消和暂停机制
- 失败任务的重试策略

### 批量导入优化
- 分批处理数据，避免内存溢出
- Elasticsearch Bulk API集成
- 并行导入和背压控制
- 导入速度监控和动态调整
- 错误数据跳过和记录机制

### 进度跟踪系统
- 实时进度百分比计算
- 导入速度和剩余时间估算
- 成功/失败/跳过记录统计
- WebSocket实时状态推送
- 导入日志详细记录

## Implementation Details

### 服务类结构
```java
@Service
@Slf4j
public class DataImportService {

    // 异步启动完整导入流程
    @Async("importTaskExecutor")
    public CompletableFuture<ImportResult> importJsonData(
        Long searchSpaceId,
        JsonSchemaAnalysis analysis,
        IndexMappingConfig indexConfig,
        List<Map<String, Object>> jsonData
    )

    // 创建Elasticsearch索引
    public boolean createElasticsearchIndex(String indexName, IndexMappingConfig config)

    // 批量导入数据
    public ImportResult bulkImportData(String indexName, List<Map<String, Object>> data)

    // 获取导入进度
    public ImportProgress getImportProgress(String taskId)

    // 取消导入任务
    public boolean cancelImportTask(String taskId)
}
```

### 导入流程设计
1. **预处理阶段**:
   - 验证搜索空间和权限
   - 检查Elasticsearch连接
   - 验证索引配置有效性
   - 初始化进度跟踪

2. **索引创建阶段**:
   - 检查索引名称冲突
   - 创建Elasticsearch索引
   - 应用映射配置
   - 验证索引创建成功

3. **数据导入阶段**:
   - 数据分批处理（每批1000条）
   - 异步批量导入执行
   - 实时进度更新
   - 错误处理和重试

4. **后处理阶段**:
   - 更新搜索空间状态
   - 生成导入统计报告
   - 清理临时文件
   - 发送完成通知

### 数据模型设计
```java
@Data
@Builder
public class ImportResult {
    private String taskId;
    private ImportStatus status;
    private int totalRecords;
    private int successRecords;
    private int failedRecords;
    private int skippedRecords;
    private long startTime;
    private long endTime;
    private String errorMessage;
    private List<ImportError> errors;
}

@Data
@Builder
public class ImportProgress {
    private String taskId;
    private ImportStatus status;
    private int totalRecords;
    private int processedRecords;
    private double progressPercentage;
    private long estimatedTimeRemaining;
    private double recordsPerSecond;
    private LocalDateTime lastUpdated;
}
```

### Elasticsearch集成
```java
@Service
@Slf4j
public class ElasticsearchIndexManager {

    private final ElasticsearchClient elasticsearchClient;

    // 创建索引
    public boolean createIndex(String indexName, Map<String, Object> mapping)

    // 检查索引是否存在
    public boolean indexExists(String indexName)

    // 删除索引
    public boolean deleteIndex(String indexName)

    // 获取索引统计信息
    public IndexStats getIndexStats(String indexName)
}
```

### 批量导入器
```java
@Component
@Slf4j
public class BulkDataImporter {

    private static final int DEFAULT_BATCH_SIZE = 1000;
    private static final int MAX_RETRY_COUNT = 3;

    // 批量导入数据
    public BulkImportResult bulkImport(String indexName, List<Map<String, Object>> documents)

    // 处理导入错误和重试
    private void handleImportErrors(BulkResponse response, List<Map<String, Object>> batch)

    // 计算最优批次大小
    private int calculateOptimalBatchSize(long documentSize, int concurrency)
}
```

### 异步配置
```java
@Configuration
@EnableAsync
public class AsyncConfig {

    @Bean("importTaskExecutor")
    public TaskExecutor importTaskExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(2);
        executor.setMaxPoolSize(5);
        executor.setQueueCapacity(100);
        executor.setThreadNamePrefix("import-task-");
        executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());
        executor.initialize();
        return executor;
    }
}
```

## Acceptance Criteria
- [ ] DataImportService核心服务创建完成
- [ ] 支持Elasticsearch索引的创建和删除
- [ ] 批量数据导入功能正常工作
- [ ] 异步任务处理和进度跟踪准确
- [ ] 错误处理和重试机制有效
- [ ] 支持大数据集导入（100万条+）
- [ ] 内存使用优化，避免OOM错误
- [ ] 导入速度达到预期（>1000条/秒）
- [ ] 单元测试覆盖核心功能
- [ ] 集成测试验证完整导入流程

## Testing Strategy
- **单元测试**:
  - 各种导入场景的功能测试
  - 错误处理和重试逻辑验证
  - 进度计算和状态更新测试
  - 批次大小优化算法测试

- **集成测试**:
  - 与Elasticsearch的完整集成
  - 真实数据的端到端导入测试
  - 异步任务执行和状态跟踪
  - 并发导入任务处理测试

- **性能测试**:
  - 100万条记录导入时间 < 20分钟
  - 内存使用稳定，无内存泄漏
  - 并发导入任务的资源竞争
  - 大文件处理的性能表现

- **压力测试**:
  - 多个大数据集同时导入
  - Elasticsearch集群负载测试
  - 系统资源耗尽场景测试

## Files to Create/Modify
- `src/main/java/com/ynet/mgmt/service/DataImportService.java`
- `src/main/java/com/ynet/mgmt/service/ElasticsearchIndexManager.java`
- `src/main/java/com/ynet/mgmt/service/BulkDataImporter.java`
- `src/main/java/com/ynet/mgmt/service/ImportProgressTracker.java`
- `src/main/java/com/ynet/mgmt/service/ImportTaskManager.java`
- `src/main/java/com/ynet/mgmt/model/import/ImportResult.java`
- `src/main/java/com/ynet/mgmt/model/import/ImportProgress.java`
- `src/main/java/com/ynet/mgmt/model/import/ImportError.java`
- `src/main/java/com/ynet/mgmt/config/AsyncConfig.java`
- `src/main/java/com/ynet/mgmt/config/ElasticsearchConfig.java`
- `src/test/java/com/ynet/mgmt/service/DataImportServiceTest.java`

## Notes
- 导入任务状态持久化到数据库，支持重启恢复
- 大文件处理采用流式读取，避免全量加载到内存
- Elasticsearch连接池配置需要优化以支持批量操作
- 考虑添加导入队列，避免并发导入过多消耗资源
- 日志级别可配置，支持详细的调试信息

## Definition of Done
- 所有代码通过code review
- 单元测试覆盖率 >= 80%
- 集成测试和性能测试通过
- 错误处理机制完善
- 监控和日志记录完整
- API文档更新完成
- 部署文档和运维指南更新